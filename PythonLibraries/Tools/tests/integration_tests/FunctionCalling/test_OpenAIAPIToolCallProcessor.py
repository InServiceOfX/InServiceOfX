from tools.FunctionCalling import (
    OpenAIAPIToolCallProcessor,
    ParseFunctionAsTool)

from tools.FunctionCalling.FunctionDefinition import Tool

from commonapi.Clients.OpenAIxGroqClient import OpenAIxGroqClient
from commonapi.Clients.OpenAIxGrokClient import OpenAIxGrokClient
from commonapi.Messages import create_user_message

from corecode.Utilities import get_environment_variable, load_environment_file
load_environment_file()

from warnings import warn

def get_horoscope(sign: str):
    """Get today's horoscope for an astrological sign.
    
    Args:
        sign: An astrological sign like Taurus or Aquarius
    """
    return f"{sign}: Next Tuesday you will befriend a baby otter."

def test_OpenAIAPIToolCallProcessor_works_on_OpenAI_API_function_tool_example_with_Groq():
    """
    Following the Function tool example in here:
    https://platform.openai.com/docs/guides/function-calling#function-tool-example
    """
    api_key = get_environment_variable("GROQ_API_KEY")

    client = OpenAIxGroqClient(api_key)
    client.clear_chat_completion_configuration()
    client.configuration.model = "llama-3.3-70b-versatile"

    function_definition = ParseFunctionAsTool.parse_for_function_definition(
        get_horoscope)

    tool_dict = Tool(function=function_definition).to_dict_for_function()

    client.configuration.tools = [tool_dict,]

    messages = [
        create_user_message("What is my horoscope? I am an Aquarius.")
    ]

    response = client.create_chat_completion(messages)

    # <class 'openai.types.chat.chat_completion.ChatCompletion'>
    # print("type(response): ", type(response))
    # AttributeError: 'ChatCompletion' object has no attribute 'output'
    #print("type(response.output): ", type(response.output))

    # Example (actual) response:
    # ChatCompletion(id='chatcmpl-531ad382-c26e-4ea6-b42e-71ee26fb4dbf', choices=[Choice(finish_reason='tool_calls', index=0, logprobs=None, message=ChatCompletionMessage(content=None, refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[ChatCompletionMessageFunctionToolCall(id='9xcbqat5q', function=Function(arguments='{"sign":"Aquarius"}', name='get_horoscope'), type='function')]))], created=1764925426, model='llama-3.3-70b-versatile', object='chat.completion', service_tier='on_demand', system_fingerprint='fp_45180df409', usage=CompletionUsage(completion_tokens=16, prompt_tokens=247, total_tokens=263, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.164911084, prompt_time=0.049820335, completion_time=0.051064094, total_time=0.100884429), usage_breakdown=None, x_groq={'id': 'req_01kbpw1jbdetkabvh3t9rst3ht', 'seed': 1645906072})
    #print("response: ", response)

    #messages += response.output

    tool_call_processor = OpenAIAPIToolCallProcessor(
        process_function_result=\
            OpenAIAPIToolCallProcessor.default_result_to_string
    )

    tool_call_processor.add_function(
        function_name=function_definition.name,
        function=get_horoscope)

    tool_call_messages = tool_call_processor.handle_possible_tool_calls(
        response.choices[0].message)

    print("tool_call_messages: ", tool_call_messages)

    for tool_call_message in tool_call_messages:
        messages.append(tool_call_message)

    # client.configuration.instructions = \
    #     "Respond only with a horoscope generated by a tool."

    # response = client.create_chat_completion(messages)

    # print(
    #     "response.model_dump_json(indent=2): ",
    #     response.model_dump_json(indent=2))

    # print("\n" + response.output_text)

def test_responses_works_on_OpenAI_API_function_tool_example_with_Grok():
    api_key = get_environment_variable("XAI_API_KEY")

    if api_key is None or api_key == "":
        warn("XAI_API_KEY is not set")
        return

    client = OpenAIxGrokClient(api_key)
    client.clear_chat_completion_configuration()
    client.configuration.model = "grok-4"

    function_definition = ParseFunctionAsTool.parse_for_function_definition(
        get_horoscope)

    tool_dict = Tool(function=function_definition).to_dict()

    client.configuration.tools = [tool_dict,]

    messages = [
        create_user_message("What is my horoscope? I am an Aquarius.")
    ]

    response = client.create_response(messages)

    # Example (actual) response:
    # Response(id='d6a807db-284e-2c20-1095-e029f2c08004', created_at=1764951387.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='grok-4-0709', object='response', output=[ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='call_93754041', name='get_horoscope', type='function_call', id='fc_d6a807db-284e-2c20-1095-e029f2c08004_0', status='completed')], parallel_tool_calls=True, temperature=None, tool_choice='auto', tools=[FunctionTool(name='get_horoscope', parameters='{"type":"object","properties":{"sign":{"type":"string","description":"An astrological sign like Taurus or Aquarius"}},"required":["sign"]}', strict=False, type='function', description="Get today's horoscope for an astrological sign.")], top_p=None, background=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary='detailed'), safety_identifier=None, service_tier=None, status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation=None, usage=ResponseUsage(input_tokens=900, input_tokens_details=InputTokensDetails(cached_tokens=834), output_tokens=143, output_tokens_details=OutputTokensDetails(reasoning_tokens=115), total_tokens=1043, num_sources_used=0, num_server_side_tools_used=0), user=None, store=True)
    # Notice tools field and output field:
    # output=[ResponseFunctionToolCall ...
    # print("response: ", response)
    # <class 'openai.types.responses.response.Response'>
    # print("type(response): ", type(response))

    # <class 'list'>
    # print("type(response.output): ", type(response.output))
    # [ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='call_37908990', name='get_horoscope', type='function_call', id='fc_af6721c9-79fe-6546-1a7a-8e0c9fb0f777_0', status='completed')]
    # print("response.output: ", response.output)
    # Don't append; you need to "flatten" the list that is response.output.
    messages += response.output

    tool_call_processor = OpenAIAPIToolCallProcessor(
        process_function_result=\
            OpenAIAPIToolCallProcessor.default_result_to_string
    )

    tool_call_processor.add_function(
        function_name=function_definition.name,
        function=get_horoscope)

    # Code to show the steps explicitly.
    # import json
    # horoscope = get_horoscope(json.loads(response.output[0].arguments))
    # function_call_output = json.dumps({"horoscope": horoscope})
    # messages.append({
    #     "type": "function_call_output",
    #     "call_id": response.output[0].call_id,
    #     "output": function_call_output
    # })

    # {"horoscope": "{'sign': 'Aquarius'}: Next Tuesday you will befriend a baby otter."}
    # print("function_call_output: ", function_call_output)
    #  <class 'str'>
    # print("type(function_call_output): ", type(function_call_output))

    tool_call_messages = tool_call_processor.handle_possible_tool_calls_as_response(
        response)

    # print("tool_call_messages: ", tool_call_messages)
    # assert len(tool_call_messages) == 1

    for tool_call_message in tool_call_messages:
        messages.append(tool_call_message)

    response = client.create_response(messages)

    # <class 'list'>
    # print("type(response.output): ", type(response.output))
    # Example (actual response.output):
    # [ResponseOutputMessage(id='msg_d2d18188-2e01-3cb9-bb74-37f190198e69', content=[ResponseOutputText(annotations=[], text="Based on today's horoscope for Aquarius: Next Tuesday you will befriend a baby otter.", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]
    # print("response.output: ", response.output)

    # 5. The model should be able to give a response!
    print("Final output:")
    # Example (actual) response.model_dump_json():
# {
#   "id": "d2d18188-2e01-3cb9-bb74-37f190198e69",
#   "created_at": 1764956391.0,
#   "error": null,
#   "incomplete_details": null,
#   "instructions": null,
#   "metadata": {},
#   "model": "grok-4-0709",
#   "object": "response",
#   "output": [
#     {
#       "id": "msg_d2d18188-2e01-3cb9-bb74-37f190198e69",
#       "content": [
#         {
#           "annotations": [],
#           "text": "Based on today's horoscope for Aquarius: Next Tuesday you will befriend a baby otter.",
#           "type": "output_text",
#           "logprobs": []
#         }
#       ],
#       "role": "assistant",
#       "status": "completed",
#       "type": "message"
#     }
#   ],
#   "parallel_tool_calls": true,
#   "temperature": null,
#   "tool_choice": "auto",
#   "tools": [
#     {
#       "name": "get_horoscope",
#       "parameters": "{\"type\":\"object\",\"properties\":{\"sign\":{\"type\":\"string\",\"description\":\"An astrological sign like Taurus or Aquarius\"}},\"required\":[\"sign\"]}",
#       "strict": false,
#       "type": "function",
#       "description": "Get today's horoscope for an astrological sign."
#     }
#   ],
#   "top_p": null,
#   "background": null,
#   "conversation": null,
#   "max_output_tokens": null,
#   "max_tool_calls": null,
#   "previous_response_id": null,
#   "prompt": null,
#   "prompt_cache_key": null,
#   "reasoning": {
#     "effort": "medium",
#     "generate_summary": null,
#     "summary": "detailed"
#   },
#   "safety_identifier": null,
#   "service_tier": null,
#   "status": "completed",
#   "text": {
#     "format": {
#       "type": "text"
#     },
#     "verbosity": null
#   },
#   "top_logprobs": null,
#   "truncation": null,
#   "usage": {
#     "input_tokens": 1182,
#     "input_tokens_details": {
#       "cached_tokens": 904
#     },
#     "output_tokens": 74,
#     "output_tokens_details": {
#       "reasoning_tokens": 56
#     },
#     "total_tokens": 1256,
#     "num_sources_used": 0,
#     "num_server_side_tools_used": 0
#   },
#   "user": null,
#   "store": true
# }
    
    # print(response.model_dump_json(indent=2))
    print("\n" + response.output_text)
