from tools.FunctionCalling import ParseFunctionAsTool
from tools.FunctionCalling.FunctionDefinition import Tool
from tools.FunctionCalling.ToolCallHandlers import ResponseProcessor

from commonapi.Clients.OpenAIxGroqClient import OpenAIxGroqClient
from commonapi.Clients.OpenAIxGrokClient import OpenAIxGrokClient
from commonapi.Messages import create_user_message

from corecode.Utilities import get_environment_variable, load_environment_file
load_environment_file()

from typing import Literal

from warnings import warn

from TestSetup.function_calling_test_setup import get_horoscope

def test_responses_works_on_OpenAI_API_function_tool_example_with_Groq():
    """
    Following the Function tool example in here:
    https://platform.openai.com/docs/guides/function-calling#function-tool-example

    Notice that the returned objects are of type Response as opposed to
    ChatCompletion.
    """
    api_key = get_environment_variable("GROQ_API_KEY")

    client = OpenAIxGroqClient(api_key)
    client.clear_chat_completion_configuration()
    client.configuration.model = "llama-3.3-70b-versatile"

    function_definition = ParseFunctionAsTool.parse_for_function_definition(
        get_horoscope)

    tool_dict = Tool(function=function_definition).to_dict()

    client.configuration.tools = [tool_dict,]

    messages = [
        create_user_message("What is my horoscope? I am an Aquarius.")
    ]

    response = client.create_response(messages)

    # <class 'openai.types.responses.response.Response'>
    #print("type(response): ", type(response))
    # type(response.output):  <class 'list'>
    #print("type(response.output): ", type(response.output))

    # Example (actual) response:
    # Response(id='resp_01kbr65cpsf74abp61g48nqgf8', created_at=1764969591.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='llama-3.3-70b-versatile', object='response', output=[ResponseReasoningItem(id='resp_01kbr65cpsf74r3vfkwsdvrydb', summary=[], type='reasoning', content=None, encrypted_content=None, status='completed'), ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='k17z2v6dw', name='get_horoscope', type='function_call', id='k17z2v6dw', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='get_horoscope', parameters={'properties': {'sign': {'description': 'An astrological sign like Taurus or Aquarius', 'type': 'string'}}, 'required': ['sign'], 'type': 'object'}, strict=None, type='function', description="Get today's horoscope for an astrological sign.")], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=None, safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation='disabled', usage=ResponseUsage(input_tokens=247, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=16, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=263), user=None, groq=None, store=False)
    #print("response: ", response)

    # Example (actual) response.output:
    # [ResponseReasoningItem(id='resp_01kbr65cpsf74r3vfkwsdvrydb', summary=[], type='reasoning', content=None, encrypted_content=None, status='completed'), ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='k17z2v6dw', name='get_horoscope', type='function_call', id='k17z2v6dw', status='completed')]
    # [ResponseReasoningItem(id='resp_01kbra77k8fe6vww0ybhkrkzgf', summary=[], type='reasoning', content=None, encrypted_content=None, status='completed'), ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='kby97en4a', name='get_horoscope', type='function_call', id='kby97en4a', status='completed')]
    # print("response.output: ", response.output)

    assert ResponseProcessor.is_function_call(response)
    assert ResponseProcessor.is_text_response(response)

    messages += response.output

    tool_call_processor = ResponseProcessor(
        process_function_result=ResponseProcessor.default_result_to_string
    )

    tool_call_processor.add_function(
        function_name=function_definition.name,
        function=get_horoscope)

    tool_call_messages = tool_call_processor.handle_possible_tool_calls(
        response)

    for tool_call_message in tool_call_messages:
        messages.append(tool_call_message)

    client.configuration.instructions = \
         "Respond only with a horoscope generated by a tool."

    response = client.create_response(messages)

    assert not ResponseProcessor.is_function_call(response)
    assert ResponseProcessor.is_text_response(response)

    # 5. The model should be able to give a response!
    # {
    #   "id": "resp_01kbra7857emmr9txzetgxx4a1",
    #   "created_at": 1764973846.0,
    #   "error": null,
    #   "incomplete_details": null,
    #   "instructions": "Respond only with a horoscope generated by a tool.",
    #   "metadata": {},
    #   "model": "llama-3.3-70b-versatile",
    #   "object": "response",
    #   "output": [
    #     {
    #       "id": "resp_01kbra7857emn8ypeagmd004d1",
    #       "summary": [],
    #       "type": "reasoning",
    #       "content": null,
    #       "encrypted_content": null,
    #       "status": "completed"
    #     },
    #     {
    #       "id": "msg_01kbra7857emntgz93zg4tdpd6",
    #       "content": [
    #         {
    #           "annotations": [],
    #           "text": "I don't have any further information on the accuracy of the statement provided, but according to the function call, the horoscope for Aquarius is: Aquarius: Next Tuesday you will befriend a baby otter.",
    #           "type": "output_text",
    #           "logprobs": null
    #         }
    #       ],
    #       "role": "assistant",
    #       "status": "completed",
    #       "type": "message"
    #     }
    #   ],
    #   "parallel_tool_calls": true,
    #   "temperature": 1.0,
    #   "tool_choice": "auto",
    #   "tools": [
    #     {
    #       "name": "get_horoscope",
    #       "parameters": {
    #         "properties": {
    #           "sign": {
    #             "description": "An astrological sign like Taurus or Aquarius",
    #             "type": "string"
    #           }
    #         },
    #         "required": [
    #           "sign"
    #         ],
    #         "type": "object"
    #       },
    #       "strict": null,
    #       "type": "function",
    #       "description": "Get today's horoscope for an astrological sign."
    #     }
    #   ],
    #   "top_p": 1.0,
    #   "background": false,
    #   "conversation": null,
    #   "max_output_tokens": null,
    #   "max_tool_calls": null,
    #   "previous_response_id": null,
    #   "prompt": null,
    #   "prompt_cache_key": null,
    #   "prompt_cache_retention": null,
    #   "reasoning": null,
    #   "safety_identifier": null,
    #   "service_tier": "default",
    #   "status": "completed",
    #   "text": {
    #     "format": {
    #       "type": "text"
    #     },
    #     "verbosity": null
    #   },
    #   "top_logprobs": null,
    #   "truncation": "disabled",
    #   "usage": {
    #     "input_tokens": 297,
    #     "input_tokens_details": {
    #       "cached_tokens": 0
    #     },
    #     "output_tokens": 45,
    #     "output_tokens_details": {
    #       "reasoning_tokens": 0
    #     },
    #     "total_tokens": 342
    #   },
    #   "user": null,
    #   "groq": null,
    #   "store": false
    # }
    # print(
    #     "response.model_dump_json(indent=2): ",
    #     response.model_dump_json(indent=2))

    # I don't have any further information on the accuracy of the statement provided, but according to the function call, the horoscope for Aquarius is: Aquarius: Next Tuesday you will befriend a baby otter.
    # print("\n" + response.output_text)

def test_responses_works_on_OpenAI_API_function_tool_example_with_Grok():
    api_key = get_environment_variable("XAI_API_KEY")

    if api_key is None or api_key == "":
        warn("XAI_API_KEY is not set")
        return

    client = OpenAIxGrokClient(api_key)
    client.clear_chat_completion_configuration()
    client.configuration.model = "grok-4"

    function_definition = ParseFunctionAsTool.parse_for_function_definition(
        get_horoscope)

    tool_dict = Tool(function=function_definition).to_dict()

    client.configuration.tools = [tool_dict,]

    messages = [
        create_user_message("What is my horoscope? I am an Aquarius.")
    ]

    response = client.create_response(messages)

    # Example (actual) response:
    # Response(id='d6a807db-284e-2c20-1095-e029f2c08004', created_at=1764951387.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='grok-4-0709', object='response', output=[ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='call_93754041', name='get_horoscope', type='function_call', id='fc_d6a807db-284e-2c20-1095-e029f2c08004_0', status='completed')], parallel_tool_calls=True, temperature=None, tool_choice='auto', tools=[FunctionTool(name='get_horoscope', parameters='{"type":"object","properties":{"sign":{"type":"string","description":"An astrological sign like Taurus or Aquarius"}},"required":["sign"]}', strict=False, type='function', description="Get today's horoscope for an astrological sign.")], top_p=None, background=None, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary='detailed'), safety_identifier=None, service_tier=None, status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation=None, usage=ResponseUsage(input_tokens=900, input_tokens_details=InputTokensDetails(cached_tokens=834), output_tokens=143, output_tokens_details=OutputTokensDetails(reasoning_tokens=115), total_tokens=1043, num_sources_used=0, num_server_side_tools_used=0), user=None, store=True)
    # Notice tools field and output field:
    # output=[ResponseFunctionToolCall ...
    # print("response: ", response)
    # <class 'openai.types.responses.response.Response'>
    # print("type(response): ", type(response))

    # <class 'list'>
    # print("type(response.output): ", type(response.output))
    # [ResponseFunctionToolCall(arguments='{"sign":"Aquarius"}', call_id='call_37908990', name='get_horoscope', type='function_call', id='fc_af6721c9-79fe-6546-1a7a-8e0c9fb0f777_0', status='completed')]
    # print("response.output: ", response.output)
    # Don't append; you need to "flatten" the list that is response.output.
    messages += response.output

    tool_call_processor = ResponseProcessor(
        process_function_result=ResponseProcessor.default_result_to_string
    )

    tool_call_processor.add_function(
        function_name=function_definition.name,
        function=get_horoscope)

    # Code to show the steps explicitly.
    # import json
    # horoscope = get_horoscope(json.loads(response.output[0].arguments))
    # function_call_output = json.dumps({"horoscope": horoscope})
    # messages.append({
    #     "type": "function_call_output",
    #     "call_id": response.output[0].call_id,
    #     "output": function_call_output
    # })

    # {"horoscope": "{'sign': 'Aquarius'}: Next Tuesday you will befriend a baby otter."}
    # print("function_call_output: ", function_call_output)
    #  <class 'str'>
    # print("type(function_call_output): ", type(function_call_output))

    tool_call_messages = tool_call_processor.handle_possible_tool_calls(response)

    # print("tool_call_messages: ", tool_call_messages)
    # assert len(tool_call_messages) == 1

    for tool_call_message in tool_call_messages:
        messages.append(tool_call_message)

    response = client.create_response(messages)

    # <class 'list'>
    # print("type(response.output): ", type(response.output))
    # Example (actual response.output):
    # [ResponseOutputMessage(id='msg_d2d18188-2e01-3cb9-bb74-37f190198e69', content=[ResponseOutputText(annotations=[], text="Based on today's horoscope for Aquarius: Next Tuesday you will befriend a baby otter.", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]
    # print("response.output: ", response.output)

    # 5. The model should be able to give a response!
    print("Final output:")
    # Example (actual) response.model_dump_json():
# {
#   "id": "d2d18188-2e01-3cb9-bb74-37f190198e69",
#   "created_at": 1764956391.0,
#   "error": null,
#   "incomplete_details": null,
#   "instructions": null,
#   "metadata": {},
#   "model": "grok-4-0709",
#   "object": "response",
#   "output": [
#     {
#       "id": "msg_d2d18188-2e01-3cb9-bb74-37f190198e69",
#       "content": [
#         {
#           "annotations": [],
#           "text": "Based on today's horoscope for Aquarius: Next Tuesday you will befriend a baby otter.",
#           "type": "output_text",
#           "logprobs": []
#         }
#       ],
#       "role": "assistant",
#       "status": "completed",
#       "type": "message"
#     }
#   ],
#   "parallel_tool_calls": true,
#   "temperature": null,
#   "tool_choice": "auto",
#   "tools": [
#     {
#       "name": "get_horoscope",
#       "parameters": "{\"type\":\"object\",\"properties\":{\"sign\":{\"type\":\"string\",\"description\":\"An astrological sign like Taurus or Aquarius\"}},\"required\":[\"sign\"]}",
#       "strict": false,
#       "type": "function",
#       "description": "Get today's horoscope for an astrological sign."
#     }
#   ],
#   "top_p": null,
#   "background": null,
#   "conversation": null,
#   "max_output_tokens": null,
#   "max_tool_calls": null,
#   "previous_response_id": null,
#   "prompt": null,
#   "prompt_cache_key": null,
#   "reasoning": {
#     "effort": "medium",
#     "generate_summary": null,
#     "summary": "detailed"
#   },
#   "safety_identifier": null,
#   "service_tier": null,
#   "status": "completed",
#   "text": {
#     "format": {
#       "type": "text"
#     },
#     "verbosity": null
#   },
#   "top_logprobs": null,
#   "truncation": null,
#   "usage": {
#     "input_tokens": 1182,
#     "input_tokens_details": {
#       "cached_tokens": 904
#     },
#     "output_tokens": 74,
#     "output_tokens_details": {
#       "reasoning_tokens": 56
#     },
#     "total_tokens": 1256,
#     "num_sources_used": 0,
#     "num_server_side_tools_used": 0
#   },
#   "user": null,
#   "store": true
# }
    
    # print(response.model_dump_json(indent=2))
    print("\n" + response.output_text)

def get_current_temperature(
    location: str,
    unit: Literal["celsius", "fahrenheit"] = "fahrenheit"):
    """Get the current temperature in a given location
    
    Args:
        location: The city and state, e.g. San Francisco, CA
    """
    temperature = 59 if unit == "fahrenheit" else 15
    return {
        "location": location,
        "temperature": temperature,
        "unit": unit,
    }

def get_current_ceiling(location: str):
    """Get the current cloud ceiling in a given location

    Args:
        location: The city and state, e.g. San Francisco, CA
    """
    return {
        "location": location,
        "ceiling": 15000,
        "ceiling_type": "broken",
        "unit": "ft",
    }

def test_responses_with_XAI_API_function_calling_example_with_Grok():
    api_key = get_environment_variable("XAI_API_KEY")

    if api_key is None or api_key == "":
        warn("XAI_API_KEY is not set")
        return

    client = OpenAIxGrokClient(api_key)
    client.clear_chat_completion_configuration()
    client.configuration.model = "grok-4"

    temperature_function_definition = \
        ParseFunctionAsTool.parse_for_function_definition(
            get_current_temperature)
    ceiling_function_definition = \
        ParseFunctionAsTool.parse_for_function_definition(get_current_ceiling)

    temperature_tool_dict = Tool(function=temperature_function_definition).to_dict()
    ceiling_tool_dict = Tool(function=ceiling_function_definition).to_dict()

    # {'name': 'get_current_temperature', 'description': 'Get the current temperature in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}, 'unit': {'type': 'string', 'description': ''}}, 'required': ['location', 'unit']}, 'type': 'function'}
    # print("temperature_tool_dict: ", temperature_tool_dict)
    # {'name': 'get_current_ceiling', 'description': 'Get the current cloud ceiling in a given location', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'The city and state, e.g. San Francisco, CA'}}, 'required': ['location']}, 'type': 'function'}
    # print("ceiling_tool_dict: ", ceiling_tool_dict)

    tool_dicts = [temperature_tool_dict, ceiling_tool_dict]

    client.configuration.tools = tool_dicts

    messages = [
        create_user_message("What's the temperature like in San Francisco?")]

    response = client.create_response(messages)

    # Is empty
    # print("response.output_text: ", response.output_text)

    assert response.output_text == ""
    assert ResponseProcessor.is_function_call(response)

    # hasattr(response, 'output_text'):  True
    # print("hasattr(response, 'output_text'): ", hasattr(response, 'output_text'))

    assert not ResponseProcessor.is_text_response(response)

    messages += response.output

    tool_call_processor = ResponseProcessor(
        process_function_result=ResponseProcessor.default_result_to_string
    )

    tool_call_processor.add_function(
        function_name=temperature_function_definition.name,
        function=get_current_temperature)

    tool_call_processor.add_function(
        function_name=ceiling_function_definition.name,
        function=get_current_ceiling)

    tool_call_messages = tool_call_processor.handle_possible_tool_calls(
        response)

    for tool_call_message in tool_call_messages:
        messages.append(tool_call_message)

    response = client.create_response(messages)

    assert not ResponseProcessor.is_function_call(response)
    assert ResponseProcessor.is_text_response(response)

    tool_call_messages = tool_call_processor.handle_possible_tool_calls(
        response)

    assert len(tool_call_messages) == 0

    print("Final output:")
    print("\n" + response.output_text)