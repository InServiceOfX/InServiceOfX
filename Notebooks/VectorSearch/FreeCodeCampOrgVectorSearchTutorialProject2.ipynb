{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4aaa9bdb",
   "metadata": {},
   "source": [
    "# Project 2: RAG with Atlast Vector Search, LangChain, OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9601009",
   "metadata": {},
   "source": [
    "[Vector Search RAG Tutorial – Combine Your Data with LLMs with Advanced Search around 39:27](https://youtu.be/JEBDfGqrAUA?si=rENOiHcL_PJyVBk6&t=2367)\n",
    "\n",
    "https://www.mongodb.com/developer/products/atlas/rag-atlas-vector-search-langchain-openai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01d5190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df915e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/InServiceOfX\n",
      "Is CoreCode directory in sys.path? False\n",
      "Is notebook directory's ancestor in sys.path? False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Make this path be the project's \"base\" directory, so we can include modules\n",
    "notebook_directory_ancestor = Path.cwd().resolve().parent.parent\n",
    "print(notebook_directory_ancestor)\n",
    "core_code_directory = notebook_directory_ancestor / \"CoreCode/\"\n",
    "data_directory = notebook_directory_ancestor / \"ThirdParty\" / \"unit_tests\" / \"SampleData/\"\n",
    "\n",
    "is_core_code_directory_in_sys_path = str(core_code_directory) in sys.path\n",
    "is_notebook_directory_ancestor_in_sys_path = str(notebook_directory_ancestor) in sys.path\n",
    "print(\"Is CoreCode directory in sys.path?\", is_core_code_directory_in_sys_path)\n",
    "print(\"Is notebook directory's ancestor in sys.path?\", is_notebook_directory_ancestor_in_sys_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e74b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_core_code_directory_in_sys_path:\n",
    "    sys.path.append(str(core_code_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0649a9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CoreCode.MongoDBInterface.connect_to_client import (connect_to_client, GetCollection)\n",
    "from CoreCode.MongoDBInterface.CreateURI import CreateURI\n",
    "from CoreCode.Utilities.LoadEnvironmentFile import load_environment_file\n",
    "load_environment_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da48bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_uri = CreateURI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06532928",
   "metadata": {},
   "source": [
    "Enter your password for MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ab0fa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your password for MongoDB: ········\n"
     ]
    }
   ],
   "source": [
    "new_uri = create_uri.prompt_password()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57b2bfde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 44.6 ms, sys: 30.1 ms, total: 74.8 ms\n",
      "Wall time: 14.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "client = connect_to_client(new_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b7e4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_collection = GetCollection(\"langchain_demo\", \"collection_of_text_blobs\")\n",
    "collection = get_collection.get_collection(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "856d8740",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/3 [00:00<?, ?it/s][nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      " 33%|███████████████                              | 1/3 [00:23<00:46, 23.11s/it][nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:28<00:00,  9.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.55 s, sys: 1.04 s, total: 3.59 s\n",
      "Wall time: 28.8 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "loader = DirectoryLoader(data_directory, glob=\"./*.txt\", show_progress=True)\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd5da7",
   "metadata": {},
   "source": [
    "Define the OpenAI Embedding Model we want to use for the source data. The embedding model is different from the language generation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356d6a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ThirdParty/langchain/libs/core/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99a94e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8833d2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a727e2",
   "metadata": {},
   "source": [
    "Initialize the VectorStore. Vectorise the text from the documents using the specified embedding model, and insert them into the specified MongoDB collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "656a9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "vector_store = MongoDBAtlasVectorSearch.from_documents(data, embedding_model, collection=collection )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edc8538",
   "metadata": {},
   "source": [
    "Start of the [`extract_information.py`](https://youtu.be/JEBDfGqrAUA?si=xVChrbyuLH6J9vRs&t=2780) script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86d62676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 1 µs, total: 6 µs\n",
      "Wall time: 10 µs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ThirdParty/langchain/libs/core/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.vectorstores.mongodb_atlas.MongoDBAtlasVectorSearch` was deprecated in langchain-community 0.0.25 and will be removed in 0.2.0. An updated version of the class exists in the langchain-mongodb package and should be used instead. To use it run `pip install -U langchain-mongodb` and import as `from langchain_mongodb import MongoDBAtlasVectorSearch`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "atlas_vector_store = MongoDBAtlasVectorSearch(collection, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "585c3096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided to pip install this here, but not in my Dockerfile. I did\n",
    "# pip install -U langchain-mongodb\n",
    "from langchain_mongodb import MongoDBAtlasVectorSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb178196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 0 ns, total: 14 µs\n",
      "Wall time: 19.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "atlas_vector_store = MongoDBAtlasVectorSearch(collection, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8d837a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7993d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_data(query):\n",
    "    # Convert question to vector using OpenAI embeddings\n",
    "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
    "    # similarity_search returns MongoDB documents most similar to the query    \n",
    "\n",
    "    docs = atlas_vector_store.similarity_search(query, K=1)\n",
    "    as_output = docs[0].page_content\n",
    "\n",
    "    # Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
    "\n",
    "    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model\n",
    "    # If it's not specified (for example like in the code below),\n",
    "    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023\n",
    "    # temperature=0 means more definitive, less creative.\n",
    "    llm = OpenAI(openai_api_key=openai_api_key, temperature=0)\n",
    "\n",
    "    # Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
    "    # Implements _get_relevant_documents which retrieves documents relevant to a query.\n",
    "    retriever = atlas_vector_store.as_retriever()\n",
    "\n",
    "    # Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
    "    # inserts them all into a prompt and passes that prompt to an LLM.\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    # Execute the chain\n",
    "\n",
    "    retriever_output = qa.run(query)\n",
    "\n",
    "    # Return Atlas Vector Search output, and output generated using RAG Architecture\n",
    "    return as_output, retriever_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7e2ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
    "        \"\"\")\n",
    "    textbox = gr.Textbox(label=\"Enter your Question:\")\n",
    "    with gr.Row():\n",
    "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
    "    with gr.Column():\n",
    "        output1 = gr.Textbox(lines=1, max_lines=10, label=\"Output with just Atlas Vector Search (returns text field as is):\")\n",
    "        output2 = gr.Textbox(lines=1, max_lines=10, label=\"Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:\")\n",
    "\n",
    "# Call query_data function upon clicking the Submit button\n",
    "\n",
    "    button.click(query_data, textbox, outputs=[output1, output2])\n",
    "\n",
    "demo.launch()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2355d358",
   "metadata": {},
   "source": [
    "## Exploring the objects we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a48e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_retriever = atlas_vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6838582b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pymongo.collection.Collection'>\n",
      "<class 'langchain_openai.embeddings.base.OpenAIEmbeddings'>\n",
      "<class 'langchain_mongodb.vectorstores.MongoDBAtlasVectorSearch'>\n",
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "print(type(collection))\n",
    "print(type(embedding_model))\n",
    "print(type(atlas_vector_store))\n",
    "print(type(example_retriever))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91492ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
